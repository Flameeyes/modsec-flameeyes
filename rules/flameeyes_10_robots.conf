# -*- apache -*-
# Copyright © 2010 Diego Elio Pettenò <flameeyes@gmail.com>
#

# First of all, reject altogether some known bad robots, usually
# marketing and malicious bots. Some of these are already listed by
# the Core Rule Set but they are not considered fatal
SecRule REQUEST_HEADERS:User-Agent "@pmFromFile flameeyes_bad_robots.data" \
	"deny,status:403,msg:'Bad crawler %{REQUEST_HEADERS:User-Agent}',phase:1,id:flameeyes-1"

# If the ModSecurity Core Rule Set is installed, it is also going to
# handle rogue robots; in this case, check the transaction variables
# to see if they caught something for us.
SecRule TX:INBOUND_TX_MSG "@contains Rogue web site crawler" \
        "phase:2,deny,status:403,msg:'ModSecurity CRS identified a bad crawler %{REQUEST_HEADERS:User-Agent}',id:flameeyes-4"

# Then remove a number of base access libraries; these usually
# indicate programs that are not sophisticated or designed enough to
# provide a specific User-Agent; this often enough is spam.
SecRule REQUEST_HEADERS:User-Agent "^(?:libwww-perl|jakarta commons-httpclient|java|python-urllib|rome client|indy library|pycurl|php)/.*" \
       "deny,status:403,msg:'Generic libraries without proper User-Agent %{REQUEST_HEADERS:User-Agent}',phase:1,id:flameeyes-2"

# Finally remove a series of User-Agents that are known to mask
# scanners; this might not be very sophisticated but works.
SecRule REQUEST_HEADERS:User-Agent "(?:xx$|^mozilla/5\.0( \(en-us\))?$|^mozilla/4\.0 \(compatible;\)$|^mozilla/4\.0 \(compatible; msie 6\.0; windows nt 5\.1\)$)" \
	"deny,status:403,msg:'Unknown scanner passing for a real browser',phase:1,id:flameeyes-3"

# Start FcRDNS verification of robots that have a way to verify their
# source; also avoid furhter work by ignoring all the other tests as
# only one can apply.
#
# http://blog.flameeyes.eu/2009/12/04/do-i-hate-bots-too-much
SecRule REQUEST_HEADERS:User-Agent "@contains googlebot" \
	"chain,deny,status:403,msg:'Fake Googlebot crawler.',phase:1,skipAfter:END_ROBOT_CHECKS"
SecRule REMOTE_HOST "!@endsWith .googlebot.com"

SecRule REQUEST_HEADERS:User-Agent "@contains feedfetcher-google" \
	"chain,deny,status:403,msg:'Fake Google feed fetcher.',phase:1,skipAfter:END_ROBOT_CHECKS"
SecRule REMOTE_HOST "!@endsWith .google.com"

SecRule REQUEST_HEADERS:User-Agent "@contains msnbot/" \
	"chain,deny,status:403,msg:'Fake msnbot crawler.',phase:1,skipAfter:END_ROBOT_CHECKS"
SecRule REMOTE_HOST "!msnbot-.*\.msn\.com"

SecRule REQUEST_HEADERS:User-Agent "@contains yahoo! slurp" \
	"chain,deny,status:403,msg:'Fake Yahoo! Slurp crawler.',phase:1,skipAfter:END_ROBOT_CHECKS"
SecRule REMOTE_HOST "!@endsWith .crawl.yahoo.net"

SecRule REQUEST_HEADERS:User-Agent "@contains yahoo pipes" \
	"chain,deny,status:403,msg:'Fake Yahoo Pipes crawler.',phase:1,skipAfter:END_ROBOT_CHECKS"
SecRule REMOTE_HOST "!\.yahoo\.(?:com|net)$"

SecRule REQUEST_HEADERS:User-Agent "@contains baiduspider" \
	"chain,deny,status:403,msg:'Fake Baiduspider crawler.',phase:1,skipAfter:END_ROBOT_CHECKS"
SecRule REMOTE_HOST "!\.crawl\.baidu\.(?:com|jp)$"

# Check for robots that don't use Accept-Encoding to receive
# compressed responses; we expect that at least one between deflate
# and gzip (the common compression techniques) is supported by
# crawlers interested in the website.
#
# http://blog.flameeyes.eu/2010/10/03/size-matters-for-crawlers-too
SecRule REQUEST_URI "@streq /robots.txt" \
	"phase:1,pass,setvar:ip.is_robot=1,expirevar:ip.previous_rbl_check:259200,skipAfter:END_ROBOT_CHECKS,nolog"
SecRule REQUEST_HEADERS:Accept-Encoding \
	"@pm deflate gzip" "phase:1,skipAfter:END_ROBOT_CHECKS,nolog"

SecRule IP:IS_ROBOT "@eq 1" \
	"phase:1,deny,status:406,msg:'Robot at %{REMOTE_ADDR} is not supporting compressed responses'"
SecMarker END_ROBOT_CHECKS
