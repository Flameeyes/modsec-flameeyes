# -*- apache -*-
# Copyright © 2010-2013 Diego Elio Pettenò <flameeyes@flameeyes.eu>
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and

# First of all, reject altogether some known bad robots, usually
# marketing and malicious bots. Some of these are already listed by
# the Core Rule Set but they are not considered fatal
SecRule REQUEST_HEADERS:User-Agent "@pmFromFile flameeyes_bad_robots.data" \
	"id:431000,phase:2,t:lowercase,deny,status:403,msg:'Known bad crawler',logdata:'%{REQUEST_HEADERS.User-Agent}'"

# Some robots are difficult to match with the pattern because they
# don't provide long-enough User-Agent headers (and thus we would be
# matching too widely). Use an explicit check for those.
SecRule REQUEST_HEADERS:User-Agent "@within obot,bah,ning" \
        "id:431010,phase:2,t:lowercase,deny,status:403,msg:'Known bad crawler',logdata:'%{REQUEST_HEADERS.User-Agent}'"

# Some marketing firms crawl from a proper FcRDNS domain, since they
# might change the name of their bot, or hide it altogether, but they
# aren't likely to change their FcRDNS, this should help us avoiding
# them.
SecRule REMOTE_HOST "@pmFromFile flameeyes_bad_firms.data" \
        "id:431020,phase:2,t:lowercase,deny,status:403,msg:'Crawler is coming from a blacklisted firm domain.',logdata:'%{REQUEST_HEADERS.User-Agent}'"

# Some bots don't really request robots.txt, because they act only on
# user requests, but are not malicious per-se. On the other hand,
# until they actually learn to use Accept-Encoding, they are better
# listed here so that they can be told to stay away.
SecRule REQUEST_HEADERS:User-Agent "@pmFromFile flameeyes_stealth_robots.data" \
        "id:431030,phase:2,setvar:ip.is_robot=1,expirevar:ip.is_robot=%{TX.IP_EXPIRATION},nolog"

# Check for robots that don't use Accept-Encoding to receive
# compressed responses; we expect that at least one between deflate
# and gzip (the common compression techniques) is supported by
# crawlers interested in the website.
#
# http://blog.flameeyes.eu/2010/10/03/size-matters-for-crawlers-too
SecRule REQUEST_URI "@streq /robots.txt" \
	"id:431040,phase:2,pass,setvar:ip.is_robot=1,expirevar:ip.is_robot=%{TX.IP_EXPIRATION},skipAfter:END_ROBOT_CHECKS,noauditlog"

SecRule REQUEST_HEADERS:Accept-Encoding "@pm deflate gzip" \
        "id:431050,phase:2,t:lowercase,skipAfter:END_ROBOT_CHECKS,nolog"

SecRule REQUEST_METHOD "@within head options" \
        "id:431060,phase:2,t:lowercase,skipAfter:END_ROBOT_CHECKS,nolog"

SecRule IP:IS_ROBOT "@eq 1" \
	"id:431070,phase:2,deny,status:406,msg:'Robot at %{REMOTE_ADDR} is not supporting compressed responses',log,noauditlog"

SecMarker END_ROBOT_CHECKS
