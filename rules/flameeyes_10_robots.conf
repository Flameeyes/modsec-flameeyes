# -*- apache -*-
# Copyright © 2010 Diego Elio Pettenò <flameeyes@gmail.com>
#

# First of all, reject altogether some known bad robots, usually
# marketing and malicious bots. Some of these are already listed by
# the Core Rule Set but they are not considered fatal
SecRule REQUEST_HEADERS:User-Agent "@pmFromFile flameeyes_bad_robots.data" \
	"phase:2,t:lowercase,deny,status:403,msg:'Bad crawler %{REQUEST_HEADERS.User-Agent}',id:flameeyes-1"

# Some robots are difficult to match with the pattern because they
# don't provide long-enough User-Agent headers. Use a regexp for
# those.
SecRule REQUEST_HEADERS:User-Agent "(?:^obot$)" \
        "phase:2,t:lowercase,deny,status:403,msg:'Bad crawler %{REQUEST_HEADERS.User-Agent}',id:flameeyes-1a"

# Some marketing firms crawl from a proper FcRDNS domain, since they
# might change the name of their bot, or hide it altogether, but they
# aren't likely to change their FcRDNS, this should help us avoiding
# them.
SecRule REMOTE_HOST "@pmFromFile flameeyes_bad_firms.data" \
        "phase:2,t:lowercase,deny,status:403,msg:'Crawler %{REQUEST_HEADERS:User-Agent} is coming from a blacklisted firm domain.'"

# If the ModSecurity Core Rule Set is installed, it is also going to
# handle rogue robots; in this case, check the transaction variables
# to see if they caught something for us.
#
# Disabled by default because it also lists wget and curl as crawlers
# (and that might no be good for general use).
#
#SecRule TX:INBOUND_TX_MSG "@contains Rogue web site crawler" \
#        "phase:2,deny,status:403,msg:'ModSecurity CRS identified a bad crawler %{REQUEST_HEADERS.User-Agent}',id:flameeyes-4"

# Then remove a number of base access libraries; these usually
# indicate programs that are not sophisticated or designed enough to
# provide a specific User-Agent; this often enough is spam.
SecRule REQUEST_HEADERS:User-Agent "^(?:libwww-perl|jakarta commons-httpclient|java|python-(?:urllib|httplib2)|rome client|indy library|pycurl|php|ruby/|gsa-crawler)" \
       "phase:2,t:lowercase,deny,status:403,msg:'Generic libraries without proper User-Agent %{REQUEST_HEADERS.User-Agent}',id:flameeyes-2"

# Finally remove a series of User-Agents that are known to mask
# scanners; this might not be very sophisticated but works.
SecRule REQUEST_HEADERS:User-Agent "(?:xx$|^mozilla/5\.0( \(en-us\))?$|^mozilla/4\.0 \(compatible;\)$|^mozilla/4\.0 \(compatible; msie 6\.0; windows nt 5\.1\)$|^mozilla/5\.0 (?:gecko|firefox)/|^firefox$^mozilla firefox)" \
	"phase:2,t:lowercase,deny,status:403,msg:'Unknown scanner passing for a real browser',id:flameeyes-3"

# Some bots don't really request robots.txt, because they act only on
# user requests, but are not malicious per-se. On the other hand,
# until they actually learn to use Accept-Encoding, they are better
# listed here so that they can be told to stay away.
SecRule REQUEST_HEADERS:User-Agent "@pmFromFile flameeyes_stealth_robots.data" \
        "phase:2,setvar:ip.is_robot=1,expirevar:ip.is_robot=%{TX.IP_EXPIRATION},nolog"

# Check for robots that don't use Accept-Encoding to receive
# compressed responses; we expect that at least one between deflate
# and gzip (the common compression techniques) is supported by
# crawlers interested in the website.
#
# http://blog.flameeyes.eu/2010/10/03/size-matters-for-crawlers-too
SecRule REQUEST_URI "@streq /robots.txt" \
	"phase:2,pass,setvar:ip.is_robot=1,expirevar:ip.is_robot=%{TX.IP_EXPIRATION},skipAfter:END_ROBOT_CHECKS"

SecRule REQUEST_HEADERS:Accept-Encoding "@pm deflate gzip" \
        "phase:2,t:lowercase,skipAfter:END_ROBOT_CHECKS,nolog"

SecRule REQUEST_METHOD "@pm head options" \
        "phase:2,t:lowercase,skipAfter:END_ROBOT_CHECKS,nolog"

SecRule IP:IS_ROBOT "@eq 1" \
	"phase:2,deny,status:406,msg:'Robot at %{REMOTE_ADDR} is not supporting compressed responses',log"

SecMarker END_ROBOT_CHECKS
